## 数据挖掘与数据化运营实战：思路、方法、技巧与应用

卢辉  -  计算机榜-数据库

> 本书是目前有关数据挖掘在数据化运营实践领域比较全面和系统的著作，也是诸多数据挖掘书籍中为数不多的穿插大量真实的实践应用案例和场景的著作，更是创造性地针对数据化运营中不同分析挖掘课题类型，推出一一对应的分析思路集锦和相应的分析技巧集成，为读者提供“菜单化”实战锦囊的著作。作者结合自己数据化运营实践中大量的项目经验，用通俗易懂的“非技术”语言和大量活泼生动的案例，围绕数据分析挖掘中的思路、方法、技巧与应用，全方位整理、总结、分享，帮助读者深刻领会和掌握“以业务为核心，以思路为重点，以分析技术为辅佐”的数据挖掘实践应用宝典。全书共19章，分为三个部分：基础篇（第1～4章）系统介绍了数据分析挖掘和数据化运营的相关背景、数据化运营中“协调配合”的核心，以及实践中常见分析项目类型；实战篇（第6～13章）主要介绍实践中常见的分析挖掘技术的实用技巧，并对大量的实践案例进行了全程分享展示；思想意识篇（第5章，第14～19章）主要是有关数据分析师的责任、意识、思维的培养和提升的总结和探索，以及一些有效的项目质控制度和经典的方法论介绍。

### 推荐序

一个数据分析师，在面对海量数据时，偶尔把自己也当做对象去分析、思考、总结，才能成为一位有那么点儿味道的数据分析师，才能不断地审视、提升分析水平，才能在数据分析的道路上走得更远。 c:21

❑ 数据分析挖掘的技巧，掌握了多少？❑ 书中的实战案例，有实操过吗？❑ 数据分析师对分析/数据的态度，你是否具备？❑ 如何有效管理团队？ c:24

在成熟阶段，数据分析师们将是一群具备了商业理解、数据分析、商业应用思考这三大核心能力的综合体。 c:197

### 前言

数据挖掘”（Data Mining） c:15

一个成功的数据挖掘商业实践，核心的因素不是技术，而是业务理解和分析思路。 c:284

作者有关数据挖掘商业实践应用的专业博客“数据挖掘 人在旅途”地址为http：//shzxqdj.blog.163.com c:63

以业务为核心，以思路为重点，以挖掘技术为辅佐 c:50

### 第1章 什么是数据化运营

数据化运营是当前企业管理和企业战略里非常热门的一个词汇。其实施的前提条件包括企业级海量数据存储的实现、精细化运营的需求（与传统的粗放型运营相对比）、数据分析和数据挖掘技术的有效应用等，并且还要得到企业决策层和管理层的支持及推动。 c:119

### 1.1 现代营销理论的发展历程

❑ 消费者的需求与愿望（Customer’s Needs and Wants）。
❑ 消费者得到满足的成本（Cost and Value to Satisfy Consumer’s Needs and Wants）。
❑ 用户购买的方便性Convenience to Buy）。
❑ 与用户的沟通交流（Communication with Consumer）。
4C理论的核心是Consumer消费者。因此，以4C理论为核心营销思想的企业营销战略又可以简称为“以消费者为中心”的营销战略。
 c:207

### 1.1.2 从4C到3P3C

上述3P3C理论有效锁定了影响运营效果的主要因素、来源，可以帮助运营人员、管理人员、数据分析人员快速区分实践中的思考维度和着力点，提高思考效率和分析效率。 c:15

### 1.2 数据化运营的主要内容

以企业级海量数据的存储和分析挖掘应用为核心支持的，企业全员参与的，以精准、细分和精细化为特点的企业运营制度和战略” c:171

针对互联网运营部门的数据化运营，具体包括“网站流量监控分析、目标用户行为研究、网站日常更新内容编辑、网络营销策划推广”等，并且，这些内容是在以企业级海量数据的存储、分析、挖掘和应用为核心技术支持的基础上，通过可量化、可细分、可预测等一系列精细化的方式来进行的。 c:114

数据化运营，其次是一种常态化的制度和流程，包括企业各个岗位和工种的数据收集和数据分析应用的框架和制度等。 c:44

数据化运营更是来自企业决策者、高层管理者的直接倡导和实质性的持续推动 c:33

其在各种场合中对数据的重要性、对数据化运营的核心竞争力价值的强调和分享，都证明了决策层是推动数据化运营的关键所在。 c:25

一部分。而这个战略的核心就是如何挖掘、分析和运用这些数据，并和全社会分享。 c:43

### 1.3 为什么要数据化运营

，当传统的营销手段、运营方法已经被同行普遍采用，当常规的营销技术、运营方法已经很难明显提升企业的运营效率时，竞争必然呼唤革命性的改变去设法提升企业的运营效率，从而提升企业的市场竞争力。时势造英雄，生逢其时的“数据化运营”恰如及时雨，登上了大数据时代企业运营的大舞台，在互联网运营的舞台上尤其光彩夺目。 c:196

### 1.4.1 企业级海量数据存储的实现

最新的存储技术为分布式数据仓库、海量数据存储技术和流计算的实时数据仓库技术。 c:32

现在，抛弃大型机+关系型数据库的模型，采用分布式的服务器集群+分布式存储的海量存储器，无论是从硬件成本、软件成本还是从硬件升级、日常维护上来讲，都是一次飞跃。 c:39

未来的数据仓库将是以流计算为主的实时数据仓库和分布式计算为主流的准实时数据仓库。 c:90

### 1.4.2 精细化运营的需求

数据化运营就是精细化运营，它强调的是更细分、更准确、更个性化。没有精细化运营的需求，就不需要数据化运营；只有数据化运营，才可以满足精细化的效益提升。 c:53

### 1.4.3 数据分析和数据挖掘技术的有效应用

数据分析和数据挖掘技术的有效应用是数据化运营的基础和技术保障，没有这个基础保障，数据化运营就是空话，就是无本之水，无缘之木。 c:42

一名出色的数据分析师必须是多面手，他不仅要具备统计技能（能熟练使用统计技术和统计工具进行分析挖掘）、数据仓库知识（比如熟悉主流数据库基本技术，可以自助取数，可以有效与数据仓库团队沟通）、数据挖掘技能（熟练掌握主流数据挖掘技术和工具），更重要的是他还要具有针对具体业务的理解能力和快速学习能力，并且要善于与业务方沟通、交流。 c:155

人才和数据是阿里巴巴集团最大的财富和最强大的核心竞争力” c:37

### 1.5 数据化运营的新现象与新发展

淘宝网上的卖家所使用的“量子恒道”就是一个非常不错的数据产品，通过使用量子恒道，淘宝卖家可以自己随时监控店铺的流量来源、买家逗留的时间、买家区域、浏览时间、各页面的流量大小、各产品的成交转化率等一系列跟店铺的实时基础数据相关的数据分析和报告，从而有效帮助卖家制定和完善相应的经营方向和经验策略。 c:74

泛BI其实就是逐渐淡化数据分析师团队作为企业数据分析应用的唯一专业队伍的印象，让更多的业务部门也逐渐参与数据分析和数据探索，让更多业务部门的员工也逐渐掌握数据分析的技能和意识。 c:48

### 2.2 统计分析与数据挖掘的主要区别

在企业的商业实战中，数据分析师分析问题、解决问题时，首先考虑的是思路，其次才会对与思路匹配的分析挖掘技术进行筛选，而不是先考虑到底是用统计技术还是用数据挖掘技术来解决这个问题。 c:125

正确的思路和方法应该是：针对具体的业务分析需求，先确定分析思路，然后根据这个分析思路去挑选和匹配合适的分析算法、分析技术，而且一个具体的分析需求一般都会有两种以上不同的思路和算法可以去探索，最后可根据验证的效果和资源匹配等一系列因素进行综合权衡，从而决定最终的思路、算法和解决方案。 c:104

析需求，先确定分析思路，然后根据这个分析思路去挑选和匹配合适的分析算法、分析技术，而且一个具体的分析需求一般都会有两种以上不同的思路和算法可以去探索，最后可根据验证的效果和资源匹配等一系列因素进行综合权衡，从而决定最终的思路、算法和解决方案。
 c:138

### 2.3 数据挖掘的主要成熟技术以及在数据化运营中的主要应用

决策树算法之所以在数据分析挖掘应用中如此流行，主要原因在于决策树的构造不需要任何领域的知识，很适合探索式的知识发掘，并且可以处理高维度的数据。在众多的数据挖掘、统计分析算法中，决策树最大的优点 c:160

### 2.3.2 神经网络

“神经网络”技术在数据化运营中的主要用途体现在：作为分类、预测问题的重要技术支持，在用户划分、行为预测、营销响应等诸多方面具有广泛的应用前景。 c:63

### 2.3.3 回归

回归（Regression）分析包括线性回归（Linear Regression），这里主要是指多元线性回归和逻辑斯蒂回归（Logistic Regression）。 c:54

### 2.3.4 关联规则

关联规则数据挖掘的主要目的是找出数据集中的频繁模式（Frequent Pattern），即多次重复出现的模式和并发关系（Cooccurrence Relationships），即同时出现的关系，频繁和并发关系也称作关联（Association）。 c:79

### 2.3.5 聚类

聚类分析的算法可以分为划分的方法（Partitioning Method）、层次的方法（Hierarchical Method）、基于密度的方法（Density-based Method）、基于网格的方法（Grid-based Method）、基于模型的方法（Model-based Method）等，其中，前面两种方法最为常用。 c:77

### 2.3.6 贝叶斯分类方法

贝叶斯分类方法（Bayesian Classifier）是非常成熟的统计学分类方法，它主要用来预测类成员间关系的可能性。 c:43

贝叶斯分类方法在数据化运营实践中主要用于分类问题的归类等应用场景。 c:24

### 2.3.7 支持向量机

支持向量机的缺点是训练数据较大，但是，它的优点也是很明显的—对于复杂的非线性的决策边界的建模能力高度准确，并且也不太容易过拟合[插图]。 c:30

支持向量机主要用在预测、分类这样的实际分析需求场景中。 c:14

### 2.3.8 主成分分析

主成分分析会通过线性组合将多个原始变量合并成若干个主成分，这样每个主成分都变成了原始变量的线性组合。这种转变的目的，一方面是可以大幅降低原始数据的维度，同时也在此过程中发现原始数据属性之间的关系。 c:60

主成分分析和因子分析在数据化运营实践中主要用于数据处理、降维、变量间关系的探索等方面，同时作为统计学里的基本而重要的分析工具和分析方法，它们在一些专题分析中也有着广泛的应用。 c:23

### 2.3.9 假设检验

在数据化运营的商业实践中，假设检验最常用的场景就是用于“运营效果的评估”上 c:32

### 2.4 互联网行业数据挖掘应用的特点

数据分析（挖掘）的周期短 c:23

数据分析（挖掘）成果的时效性明显变短。 c:29

互联网行业新技术、新应用、新模式的更新换代相比于传统行业而言更加迅速、周期更短、更加具有颠覆性，相应地对数据分析挖掘的应用需求也更为苛刻，且要多样化。 c:23

### 3.1 目标客户的特征分析

目标客户的特征分析几乎是数据化运营企业实践中最普遍、频率最高的业务分析需求之一，原因在于数据化运营的第一步（最基础的步骤）就是要找准你的目标客户、目标受众，然后才是相应的运营方案、个性化的产品与服务等。 c:58

具体来说，本项目先要从企业历史数据中寻找有在线交易历史的买卖双方，在线行为活跃的用户，以及相应的一些网站行为、捆绑了某知名的第三方支付工具的用户等，然后根据这些行为字段和模拟的人群，去分析我们期望的目标客户特征，在通过历史数据仓库的对比后，准确掌握该目标群体的规模和层次，从而提交运营业务团队正式运营。 c:39

### 3.3 运营群体的活跃度定义

1）活跃度的组成指标应该是该业务场景中最核心的行为因素。2）衡量活跃度的定义合适与否的重要判断依据是其能否有效回答业务需求的终极目标。 c:80

项目最终活跃度的定义是否合适，是否满足业务需求，一个最重要的评估依据就是按照该活跃度定义出来的活跃用户群体里，可以覆盖多少实际的PM产品付费用户。 c:41

活跃度的定义所涉及的统计技术主要有两个，一个是主成分分析，另一个是数据的标准化。 c:59

### 3.4 用户路径分析

路径分析常用的分析技术有两类，一类是有算法支持的，另一类是严格按照步骤顺序遍历主要路径的。 c:61

### 3.5 交叉销售模型

一旦客户购买了商品（或者成为付费用户），企业就会想方设法保留和延长这些客户在企业的生命周期和客户的利润贡献，一般会有两个运营选择方向，一是延缓客户流失，让客户尽可能长久地留存，在该场景下，通常就是客户流失预警模型发挥作用，利用流失预警模型，提前锁定最可能流失的有价值的用户，然后客户服务团队采用各种客户关怀措施，尽量挽留客户，从而最终降低客户流失率；二是让客户消费更多的商品和服务，从而更大地提升客户的商业价值，挖掘客户利润，这种尽量挖掘客户利润的说法在以客户为中心的激烈竞争的2.0时代显得有些赤裸裸，所以，更加温和的说法就是通过数据分析挖掘，找出客户进一步的消费需求（潜在需求），从而更好及更主动地引导、满足、迎合客户需求，创造企业和客户的双赢。在这第二类场景中，涉及的主要应用模型就是交叉销售模型。
 c:75

一是按照关联技术（Association Analysis），也即通常所说的购物篮分析，发现那些有较大可能被一起采购的商品，将它们进行有针对性的促销和捆绑，这就是交叉销售；二是借鉴响应模型的思路，为某几种重要商品分别建立预测模型，对潜在消费者通过这些特定预测模型进行过滤，然后针对最有可能的前5%的消费者进行精确的营销推广；三是仍然借鉴预测响应模型的思路，让重要商品两两组合，找出那些最有可能消费的潜在客户；四是通过决策树清晰的树状规则，发现基于具体数据资源的具体规则（有的多，有的少），国外很多营销方案的制订和执行实际上都是通过这种方式找到灵感和思路的。 c:74

相应的建模技术主要包括关联分析（Association Analysis）、序列分析（Sequence Analysis），即在关联分析的基础上，增加了先后顺序的考虑，以及预测（响应、分类）模型技术，诸如逻辑回归、决策树等。 c:26

A产品与B产品都是公司SAAS系列产品线上的重点产品，经过分析发现两者付费用户的重合度高达40%，现在运营方需要一个数据分析解决方案，可以有效识别出最可能在消费A产品的基础上也消费B产品的潜在优质用户。 c:20

### 3.6 信息质量模型

互联网行业的信息质量模型所应用的场合主要包括商品Offer质量优化、网上店铺质量优化、网上论坛的发帖质量优化、违禁信息的过滤优化等，凡是涉及信息质量监控和优化的场景都是适用（或借鉴）信息质量模型的解决方案的。 c:35

### 3.7 服务保障模型

服务保障模型主要是站在为客户服务的角度来说的，出发点是为了让客户（平台的卖家）更好地做生意，达成更多的交易，我们（平台）应该为他们提供哪些有价值的服务去支持、保障卖家生意的发展，这里的服务方向就可以有很多的空间去想象了。 c:20

### 3.8 用户（买家、卖家）分层模型

客户服务团队需要根据分层模型来针对不同的群体提供不同的说辞和相应的服务套餐；企业管理层需要基于在线交易卖家数量来形成以其为核心的卖家分层进化视图；运营团队需要通过客户分层模型来指导相应的运营方案的制订和执行，从而提高运营效率和付费转化率等。 c:33

### 3.9 卖家（买家）交易模型

自动匹配（预测）买家感兴趣的商品（即商品推荐模型）、交易漏斗分析（找出交易环节的流失漏斗，帮助提升交易效率）、买家细分（帮助提供个性化的商品和服务）、优化交易路径设计（提升买家消费体验）等。 c:46

### 3.11.2 关联规则

Apriori算法。该算法主要包含两个步骤：首先找出数据集中所有的频繁项集，这些项集出现的频繁性要大于或等于最小支持度；然后根据频繁项集产生强关联规则，这些规则必须满足最小支持度和最小置信度。 c:22

### 3.11.3 协同过滤算法

启发式协同过滤算法主要包含3个步骤：1）收集用户偏好信息；2）寻找相似的商品或者用户；3）产生推荐。 c:35

基于用户（User-Based）的协同过滤算法首先要根据用户历史行为信息，寻找与新用户相似的其他用户；同时，根据这些相似用户对其他项的评价信息预测当前新用户可能喜欢的项。 c:19

在协同过滤中，一个重要的环节就是如何选择合适的相似度计算方法，常用的两种相似度计算方法包括皮尔逊相关系数和余弦相似度等。 c:27

### 3.12 数据产品

数据产品就是自动化、产品化了数据分析师的一部分常规工作，让系统部分取代数据分析师的劳动。 c:19

### 4.1.3 策划和执行精细化运营方案

运营方案的策划和执行是运营人员的专长和专业，其中涉及了比较多的专业技能，包括营销与推广的能力、内容创造与文案编写的能力、HTML￼编写能力、简单的工具绘图与审美能力（比如熟悉并掌握页面编辑工具Axure RP，熟悉并掌握Demo绘图工具Dreamweaver等）、用户消费心理的揣摩与把握等，甚至包括经济学的基本原理 c:15

运营团队要根据数据分析师提供的目标用户特征和运营受众的规模等目标群体策划一个运营方案，这个方案包括运营计划书、文案的创作、运营刺激方案的制订、活动页面的框架、为配合效果统计的前期页面布点（中间需要与网站、数据仓库团队协调）、活动页面的设计定稿（中间要与UED团队协调）、订购流程的优化（中间需要与CRM团队协调）、运营活动的上线（中间需要与IM即时通信团队、网站技术团队协调）、活动效果的监控（中间需要与数据分析团队、数据仓库团队协调）、活动节奏把控和页面切换、活动后的总结、讨论、反馈等。 c:25

### 4.1.4 跟踪运营效果、反馈和总结

在运营效果评估中，常用的方法是AB Test，即通过对相似群体不同运营方案实施后的效果进行对比，来评价不同运营方案的运营价值和优缺点等） c:31

### 第5章 分析师常见的错误观念和对治的管理策略

影响数据挖掘模型和数据分析成果、价值的因素很多，除了技术方面的因素（包括算法、数据质量、企业硬件设施等）之外，还应该包括数据分析师本人对于数据分析的思想观念、对于数据和数据分析的态度，以及数据分析师所具有的商业意识及商业敏感度，更包括企业层面的数据化运营的意识和氛围，从某种意义上来说，后面的几个因素对数据分析成果和价值的影响要远远超过纯技术层面的因素的影响。 c:88

### 5.1 轻视业务论

数据挖掘的本质是来源于业务需求、服务于业务需求 c:22

让数据分析师经常阶段性地把办公桌搬到对口的业务团队里，与业务团队坐在一起办公。 c:16

### 5.2 技术万能论

关联关系跟因果关系是两回事，数据分析挖掘技术是发现不了因果关系的。 c:40

### 5.4 建模与应用两段论

对于数据分析师的考核和考评不是基于模型（或者分析报告和建议）本身，而是基于模型（项目）业务落地应用后的实际效果和业务反馈 c:32

### 5.6 幸福的家庭都是相似的，不幸的家庭各有各的不幸

大道至简，万法归宗，生活的智慧当然也是数据分析挖掘的智慧。 c:11

### 6.3 制定需求分析框架和分析计划

❑ 分析思路的大致描述。 c:12

### 6.4 抽取样本数据、熟悉数据、数据清洗和摸底

通过输入变量之间的相关性分析，找出潜在共线性问题的相关输入变量，对于高度线性相关的变量只保留一个。 c:24

### 6.5 按计划初步搭建挖掘模型

尝试不同的挖掘算法和分析方法，并比较不同方案的效果、效率和稳定性 c:29

### 7.2 如何有效地优化模型

通过前期的初步建模和数据熟悉，是否有新的发现，甚至能颠覆之前的业务推测或业务直觉呢 c:20

### 7.2.2 从建模的技术思路上优化

细分建模有时候会通过故意漏掉一小部分目标用户，从而可以针对剩下的绝大多数目标用户进行更有效的预测。 c:27

### 7.2.3 从建模的技术技巧上优化

业务思路上的优化比建模技术思路上的优化更重要，而建模技术思路上的优化又比单纯的建模技巧的优化更重要。 c:36

### 7.3 如何思考优化的限度

数据化运营实践中的数据分析和数据挖掘非常强调时效性，在业务需求给出的有限时间里完成优化并投入应用。 c:13

从投入与产出的对比来考虑是思考适度的另一个主要思路。 c:11

### 7.4.2 ROC曲线

真正率的增加是以假正率的增加为代价的，ROC曲线下面的面积就是比较模型准确度的指标和依据。面积大的模型对应的模型准确度要高，也就是要择优应用的模型。 c:15

### 8.1 数据的抽取要正确反映业务需求

真正熟悉业务背景，这是确保数据抽取能正确反映业务需求的王道。 c:12

❑ 确保抽取的数据所对应的当时业务背景，与现在的业务需求即将应用的业务背景没有明显的重大改变。 c:17

### 8.4.1 缺失值的常见处理方法

数据分析师首先应该知道数据缺失的原因，只有知根知底，才可以从容、正确地处理缺失值。 c:22

直接删除带有缺失值的数据元组（或观察对象） c:15

直接删除带有缺失值的观察对象的方法只适用于建模样本里缺失值比例很少，并且后期打分应用中的数据的缺失值比例也很少的情况。 c:18

对于次序型变量（Ordinal）和区间型变量（Interval）而言，用中间值、众数、最大值、最小值、用户定义的任意其他值、平均值或仅针对区间型变量来代替缺失值。 c:17

### 8.4.2 异常值的判断和处理

对于类别型变量（Category）来说，如果某个类别值出现的频率太小，太稀有，就可能是异常值 c:14

### 8.5 数据转换

数据转换主要可以分为以下四大类：
❑ 产生衍生变量。
❑ 改善变量分布特征的转换，这里主要指对不对称分布（Skewed Distributions）所进行的转换。
❑ 区间型变量的分箱转换。
❑ 针对区间型变量进行的标准化操作。 c:61

### 8.5.2 改善变量分布的转换

常见的改善分布的转换措施如下：❑ 取对数（Log）。❑ 开平方根（Square Root）。❑ 取倒数（Inverse）。❑ 开平方（Square）。❑ 取指数（Exponential）。 c:29

### 8.5.4 数据的标准化

最简单的数据标准化转换是Min-Max标准化，也叫离差标准化，是对原始数据进行线性变换，使得结果在［0，1］区间，其转换公式如下：
￼
 c:22

### 8.6.1 为什么要筛选有效的输入变量

筛选有效的输入变量是提高模型稳定性的需要。 c:12

筛选有效的输入变量是提高模型预测能力的需要 c:12

筛选有效的输入变量也是提高运算速度和运算效率的需要。 c:13

### 8.6.5 卡方检验

卡方检验（Chi-Square Statistics）在统计学里属于非参数检验，主要用来度量类别型变量，包括次序型变量等定性变量之间的关联性以及比较两个或两个以上的样本率。其基本思想就是比较理论频数和实际频数的吻合程度或拟合 c:26

### 8.6.6 IV和WOE

在应用IV和WOE的时候，需要把区间型自变量转换成类别型（次序型）自变量，同时要强调的是目标变量必须是二元变量（Binary），这两点是应用IV和WOE的前提条件。 c:21

### 8.6.9 最后的准则

既要贯彻落实上述种种有效的筛选输入变量的方法和原理，又要在数据挖掘商业实战中综合考虑诸多环境因素和制约条件，并加以权衡和折中，这就是筛选输入变量的方法和原理中最后的准则。 c:13

### 9.1 聚类分析的典型应用场景

探测、发现孤立点、异常值 c:24

### 9.2.1 划分方法

最常见的终止条件是误差平方和（SSE）局部最小 c:13

### 9.2.3 基于密度的方法

传统的聚类算法都是基于对象之间的距离，即距离作为相似性的描述指标进行聚类划分，但是这些基于距离的方法只能发现球状类型的数据，而对于非球状类型的数据来说，只根据距离来描述和判断是不够的 c:18

### 9.3.1 如何处理数据噪声和异常值

直接删除那些比其他任何数据点都要远离聚类中心点的异常值 c:15

随机抽样的方法也可以较好地规避数据噪声的影响。 c:16

### 9.3.3 聚类变量的少而精

紧紧围绕具体分析目的和业务需求挑选聚类变量。 c:13

这个案例主要是想说明，对于任何具体的聚类项目，都应该事先在脑海里有一些相应的基本核心字段可以与该项目相匹配，而不能不管是什么项目、什么任务、什么目的，一股脑把所有变量统统放进去，这种胡子眉毛一把抓的做派是没有任何意义的。 c:13

通过相关性检测，可防止相关性高的变量同时进入聚类计算 c:21

### 9.4.1 聚类的核心指标与非聚类的业务指标相辅相成

一方面坚持参与聚类的变量少而精的原则，另一方面把非聚类的业务指标与聚类结果一起拿来分析、提炼、挖掘，这种相辅相成的做法在聚类分析的应用实践中已经得到了普遍的认可和采用。 c:28

### 9.4.3 个性化推荐的应用

通过历史数据对该商品大类的买家进行聚类分析，找出不同小类目的买家细分群体（聚类结果），然后用这个聚类模型去判别这个新的买家最可能属于哪个细分群体，再去匹配跟该细分群体最相近的卖家或者卖家的商品小类目，这就是聚类思想在个性化推荐中的应用思路。 c:27

### 10.1 神经网络技术的实践应用和注意事项

简单来讲，神经网络是一组互相连接的输入/输出单元，其中每个连接都会与一个权重相关联。在学习阶段，通过调整这些连接的权重，就能够预测输入观察值的正确类标号。因此可以理解为人工神经网络是由大量神经元通过丰富完善的连接、抽象、简化和模拟而形成的一种信息处理系统。
 c:24

### 10.2.1 决策树的原理和核心要素

决策树算法的核心是在对每个结点进行测试后，选择最佳的属性，并且对决策树进行剪枝处理。 c:19

所谓先剪枝，就是决策树生长之前，就人为定好树的层数，以及每个结点所允许的最少的样本数量等，而且在给定的结点不再分裂。
 c:25

### 10.2.2 CHAID算法

卡方检验适用于类别型变量的检验，如果自变量是区间型的变量（Interval），CHAID改用F检验。 c:13

### 11.1.1 寻找目标用户

针对类似的虚拟分析，主要通过产品的相关功能、卖点来模拟相应的行为特征和属性特征 c:18

### 11.1.2 寻找运营的抓手

运营抓手就是指通过运营的方式可以用于改善和提升客户满意度的一些特定行为字段 c:25

所有的被动行为都不是用户单方面努力就可以明显改善和提升的，它取决于交易双方的多种因素。换句话说，被动行为是无法通过运营（服务）的手段有效提升和改善的。 c:11

### 11.1.3 用户群体细分的依据

用户群体的细分，可以基于单纯的运营抓手细分，也可以基于纯粹的统计分析来找出最可能显著区别于不同群体的特征字段，还可以两者兼顾、包容并蓄。 c:11

### 11.2.1 3种划分的区别

预先定义的划分、数据分析的划分、复合的划分 c:22

### 12.1 为什么要做运营效果分析

运营工作到底有没有带来业务提升的效果？业务提升的效果是否显著？运营工作的效率如何？换成具体的业务实践场景，那就是本次运营活动对于商业目标，比如提升活跃客户数量有没有效果？效果多大？相比不做运营活动来说，本次运营活动的转化率多大？提升多少？
 c:18

### 12.2.2 假设检验的基本思想

小概率事件在一次试验中发生的概率则被称为显著性水平。 c:14

### 12.2.3 T检验概述

T检验主要用以检验两组样本的均值相等的原假设。 c:13

### 12.2.11 卡方检验

主要是比较两个和两个以上的样本率（构成比例），以及对两个分类变量的关联性进行分析，其根本思想是比较理论频数和实际频数的吻合程度或者拟合度。 c:17

### 16.3 一个基本的方法论

做假设、定标准、做比较、看趋势、观全局、辨真伪、下结论。 c:42

### 19.1 SEMMA方法论

SEMMA这5个英文字母分别代表Sample（数据取样）、Explore（数据探索）、Modify（数据调整）、Model（模式化）、Assess（评价与评估）这5个核心环节。这5个环节可以按照SEMMA的顺序流转，在适当情况下各环节之间也可以相互流转， c:34
